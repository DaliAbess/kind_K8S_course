# Kubernetes Lab #6 â€” Horizontal Pod Autoscaling (HPA)

## ðŸ“– What is Horizontal Pod Autoscaler (HPA)?

The **Horizontal Pod Autoscaler** is a Kubernetes resource that automatically adjusts the number of pod replicas in a deployment, replica set, or stateful set based on observed metrics like CPU utilization, memory usage, or custom metrics.

### Key Concepts

**Horizontal vs Vertical Scaling:**
- **Horizontal Scaling (HPA)**: Increases the *number* of pod replicas â†’ More pods handling the load
- **Vertical Scaling (VPA)**: Increases the *resources* (CPU/memory) of existing pods â†’ Bigger pods

**How HPA Works:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HPA Control Loop                          â”‚
â”‚                                                              â”‚
â”‚  1. Metrics Server collects pod metrics (CPU/Memory)        â”‚
â”‚           â†“                                                  â”‚
â”‚  2. HPA Controller queries current metrics                  â”‚
â”‚           â†“                                                  â”‚
â”‚  3. Compares current vs target utilization                  â”‚
â”‚           â†“                                                  â”‚
â”‚  4. Calculates desired replica count                        â”‚
â”‚           â†“                                                  â”‚
â”‚  5. Updates Deployment/ReplicaSet spec                      â”‚
â”‚           â†“                                                  â”‚
â”‚  6. Wait (default: 15s) â†’ Repeat                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### HPA Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User/App   â”‚ â† Sends requests
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kubernetes Cluster                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      monitors      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Metrics  â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ HPA Controllerâ”‚         â”‚
â”‚  â”‚  Server  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                            â”‚                  â”‚
â”‚       â”‚                                  â”‚ scales           â”‚
â”‚       â”‚ collects metrics                 â†“                  â”‚
â”‚       â†“                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   Deployment    â”‚         â”‚
â”‚  â”‚  Pod  â”‚  Pod    â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   (hpa-demo)    â”‚         â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”  â”‚  creates  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚  â”‚ â”‚Appâ”‚ â”‚ â”‚Appâ”‚  â”‚                                        â”‚
â”‚  â”‚ â””â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”˜  â”‚                                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚   Current: 2 replicas                                       â”‚
â”‚   Target: 50% CPU                                           â”‚
â”‚   Max: 5 replicas                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Scaling Decision Formula

HPA uses this formula to determine the desired number of replicas:

```
desiredReplicas = ceil[currentReplicas Ã— (currentMetric / targetMetric)]
```

**Example:**
- Current replicas: 2
- Current CPU utilization: 80%
- Target CPU utilization: 50%

```
desiredReplicas = ceil[2 Ã— (80 / 50)] = ceil[3.2] = 4 pods
```

### Scaling Behavior Timeline

```
Time  â”‚ CPU    â”‚ Action          â”‚ Pods
â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€
00:00 â”‚  20%   â”‚ Normal          â”‚  1
00:30 â”‚  65%   â”‚ Scale up        â”‚  2
01:00 â”‚  85%   â”‚ Scale up        â”‚  3
01:30 â”‚  90%   â”‚ Scale up        â”‚  4
02:00 â”‚  55%   â”‚ Scale up        â”‚  5 (max)
â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€
02:30 â”‚  45%   â”‚ Wait (cooldown) â”‚  5
03:00 â”‚  30%   â”‚ Scale down      â”‚  4
05:00 â”‚  25%   â”‚ Scale down      â”‚  3
08:00 â”‚  15%   â”‚ Scale down      â”‚  2
12:00 â”‚  10%   â”‚ Scale down      â”‚  1
```

**Note:** Scale-down is slower (default: 5 min stabilization) to prevent flapping.

### HPA Configuration Parameters

| Parameter | Description | Example |
|-----------|-------------|---------|
| `minReplicas` | Minimum pods to maintain | `1` |
| `maxReplicas` | Maximum pods to scale to | `5` |
| `targetCPUUtilization` | Target average CPU % | `50%` |
| `scaleTargetRef` | Resource to scale | `Deployment/hpa-demo` |
| `behavior` (optional) | Custom scaling policies | Scale down: 1 pod/min |

### Why HPA Matters

âœ… **Cost Efficiency**: Only run pods you need  
âœ… **Performance**: Handle traffic spikes automatically  
âœ… **Reliability**: Maintain service during load variations  
âœ… **Operations**: Reduce manual intervention

---

## ðŸŽ¯ Goal

- Configure a Horizontal Pod Autoscaler (HPA) to automatically scale pods based on CPU usage
- Simulate CPU load and observe scaling behavior

## ðŸ§© Skills

- Scaling automation
- Metrics utilization
- Performance tuning
- Observability

## ðŸ§° Prerequisites

Before running this lab, ensure the metrics server is installed:

```bash
kubectl get pods -n kube-system | grep metrics
```

If no metrics server is installed:

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

Verify it's working:

```bash
kubectl top nodes
kubectl top pods
```

You should see CPU/memory usage values.

## Step 1 â€” App That Generates CPU Load

### File: `hpa-app.js`

```javascript
const express = require('express');
const app = express();
const port = 8080;

app.get('/', (req, res) => res.send('HPA demo is running ðŸš€'));

app.get('/load', (req, res) => {
  const end = Date.now() + 15000; // busy loop for 15 seconds
  while (Date.now() < end) {
    Math.sqrt(Math.random());
  }
  res.send('Generated CPU load for 15s ðŸ”¥');
});

app.listen(port, () => console.log(`HPA app listening on port ${port}`));
```

### File: `Dockerfile`

```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY hpa-app.js .
RUN npm install express
EXPOSE 8080
CMD ["node", "hpa-app.js"]
```

### Build and Push

```bash
docker build -t <your-dockerhub-user>/hpa-demo:v1 .
docker push <your-dockerhub-user>/hpa-demo:v1
```

## Step 2 â€” Deployment & Service

### File: `deployment-hpa.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpa-demo
  labels:
    app: hpa-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hpa-demo
  template:
    metadata:
      labels:
        app: hpa-demo
    spec:
      containers:
      - name: hpa-demo
        image: <your-dockerhub-user>/hpa-demo:v1
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: 100m
          limits:
            cpu: 200m
```

### File: `svc-hpa.yaml`

```yaml
apiVersion: v1
kind: Service
metadata:
  name: hpa-demo-svc
spec:
  selector:
    app: hpa-demo
  ports:
    - port: 80
      targetPort: 8080
  type: ClusterIP
```

### Apply

```bash
kubectl apply -f deployment-hpa.yaml
kubectl apply -f svc-hpa.yaml
```

## Step 3 â€” Create the Horizontal Pod Autoscaler

### File: `hpa.yaml`

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: hpa-demo
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: hpa-demo
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
```

### Apply

```bash
kubectl apply -f hpa.yaml
kubectl get hpa
```

You should see:

```
NAME       REFERENCE            TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo  10%/50%   1         5         1          1m
```

## Step 4 â€” Generate Load

Let's simulate CPU load so the HPA scales up.

### Run a test Pod

```bash
kubectl run loader --image=busybox -- /bin/sh -c "while true; do wget -q -O- http://hpa-demo-svc/load; done"
```

### Watch scaling

```bash
kubectl get hpa -w
```

You'll see CPU utilization rising:

```
TARGETS   80%/50%   â†’   120%/50%
```

And soon:

```
REPLICAS: 3 â†’ 4 â†’ 5
```

You can also check:

```bash
kubectl get pods -l app=hpa-demo
```

You'll see more replicas created dynamically.

## Step 5 â€” Observe Cooldown

After you stop the load generator:

```bash
kubectl delete pod loader
```

Wait a few minutes and watch:

```bash
kubectl get hpa -w
```

Pods will gradually scale back down to 1.

## Step 6 â€” Cleanup

```bash
kubectl delete -f hpa.yaml
kubectl delete -f deployment-hpa.yaml
kubectl delete -f svc-hpa.yaml
```

## ðŸ’¡ Tuning Tips

- For fast-reacting apps, use `--horizontal-pod-autoscaler-sync-period=10s` (controller manager flag)
- Combine with Cluster Autoscaler for node-level scaling
- Monitor with `kubectl top pods` and `kubectl describe hpa`

## âœ… What You've Mastered

- CPU metrics collection
- Autoscaling based on utilization
- Scaling behavior under load and cooldown
- HPA configuration and monitoring
- Load testing and performance observation

## ðŸ“š Key Concepts

**minReplicas**: Minimum number of pods to maintain  
**maxReplicas**: Maximum number of pods to scale to  
**averageUtilization**: Target CPU percentage that triggers scaling  
**scaleTargetRef**: The deployment/resource to scale
