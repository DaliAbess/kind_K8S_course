# Kubernetes Lab #5 â€” Resource Requests & Limits (Performance & Scheduling)

## ğŸ“– What are Resource Requests & Limits?

Resource requests and limits are mechanisms in Kubernetes that control how much CPU and memory containers can consume. They ensure fair resource distribution, prevent resource starvation, and maintain cluster stability.

### The Two Key Concepts

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RESOURCE REQUESTS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  What: Minimum guaranteed resources for a container            â”‚
â”‚  Purpose: Used by the scheduler to decide pod placement        â”‚
â”‚  Guarantee: Pod will always have AT LEAST this much            â”‚
â”‚  Example: requests.cpu: "100m", requests.memory: "128Mi"       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RESOURCE LIMITS                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  What: Maximum resources a container can use                   â”‚
â”‚  Purpose: Prevents containers from consuming excessive resourcesâ”‚
â”‚  Enforcement: Container is throttled (CPU) or killed (Memory)  â”‚
â”‚  Example: limits.cpu: "200m", limits.memory: "256Mi"           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Resource Units

**CPU (measured in cores):**
```
1 CPU = 1000m (millicores/millicpu)

Examples:
â”œâ”€ "100m"  = 0.1 CPU core (10% of one core)
â”œâ”€ "500m"  = 0.5 CPU core (half a core)
â”œâ”€ "1"     = 1 full CPU core
â””â”€ "2000m" = 2 CPU cores
```

**Memory (measured in bytes):**
```
Units:
â”œâ”€ Mi = Mebibytes (1 Mi = 1024 Ki = 1,048,576 bytes)
â”œâ”€ Gi = Gibibytes (1 Gi = 1024 Mi)
â”œâ”€ M  = Megabytes (1 M = 1000 K = 1,000,000 bytes)
â””â”€ G  = Gigabytes (1 G = 1000 M)

Examples:
â”œâ”€ "128Mi" = 134,217,728 bytes
â”œâ”€ "256Mi" = 268,435,456 bytes
â””â”€ "1Gi"   = 1,073,741,824 bytes
```

### How Scheduling Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLUSTER NODES                                  â”‚
â”‚                                                                   â”‚
â”‚  Node A                 Node B                 Node C             â”‚
â”‚  â”œâ”€ Total: 4 CPU       â”œâ”€ Total: 4 CPU       â”œâ”€ Total: 2 CPU    â”‚
â”‚  â”œâ”€ Total: 8Gi RAM     â”œâ”€ Total: 8Gi RAM     â”œâ”€ Total: 4Gi RAM  â”‚
â”‚  â”œâ”€ Used:  2 CPU       â”œâ”€ Used:  3.5 CPU     â”œâ”€ Used:  1.8 CPU  â”‚
â”‚  â””â”€ Used:  4Gi RAM     â””â”€ Used:  6Gi RAM     â””â”€ Used:  3Gi RAM  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–²
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Kubernetes Scheduler â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                        Evaluates Request
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NEW POD REQUEST                                â”‚
â”‚  Requests: CPU: 1, Memory: 2Gi                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Scheduler Decision:
â”œâ”€ Node A: âœ… Available (2 CPU, 4Gi remaining)
â”œâ”€ Node B: âŒ Not enough CPU (only 0.5 CPU remaining)
â””â”€ Node C: âŒ Not enough resources (0.2 CPU, 1Gi remaining)

Result: Pod scheduled to Node A
```

### Resource Behavior: Requests vs Limits

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Container Resource Lifecycle                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Configuration:
  requests:
    cpu: "100m"
    memory: "128Mi"
  limits:
    cpu: "200m"
    memory: "256Mi"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CPU USAGE                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  200m â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€LIMITâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚       â”‚                          â–²                               â”‚
â”‚  150m â”‚                      â”Œâ”€â”€â”€â”´â”€â”€â”€â”                          â”‚
â”‚       â”‚                      â”‚THROTTLEâ”‚                         â”‚
â”‚  100m â”‚â”€â”€REQUESTâ”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚       â”‚  â–²        â”‚ Normal Usage                                â”‚
â”‚   50m â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”                                          â”‚
â”‚       â”‚  â”‚  â”‚        â”‚                                          â”‚
â”‚    0m â”‚â”€â”€â”´â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚         Guaranteed   Can use up to limit   Gets throttled       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MEMORY USAGE                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  256Miâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€LIMITâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚       â”‚                          â–²                               â”‚
â”‚  200Miâ”‚                      â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”                         â”‚
â”‚       â”‚                      â”‚OOMKilledâ”‚                        â”‚
â”‚  128Miâ”‚â”€â”€REQUESTâ”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚       â”‚  â–²        â”‚ Normal Usage                                â”‚
â”‚   64Miâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”                                          â”‚
â”‚       â”‚  â”‚  â”‚        â”‚                                          â”‚
â”‚    0Miâ”‚â”€â”€â”´â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚         Guaranteed   Can grow to limit    Pod terminated!       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Quality of Service (QoS) Classes

Kubernetes assigns QoS classes based on requests and limits:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GUARANTEED                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Condition: requests = limits for ALL resources                 â”‚
â”‚  Priority: HIGHEST                                              â”‚
â”‚  Eviction: Last to be evicted                                   â”‚
â”‚                                                                  â”‚
â”‚  resources:                                                     â”‚
â”‚    requests:                                                    â”‚
â”‚      cpu: "500m"                                                â”‚
â”‚      memory: "256Mi"                                            â”‚
â”‚    limits:                                                      â”‚
â”‚      cpu: "500m"          â—„â”€â”€ Same as request                  â”‚
â”‚      memory: "256Mi"      â—„â”€â”€ Same as request                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       BURSTABLE                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Condition: requests < limits OR only requests set              â”‚
â”‚  Priority: MEDIUM                                               â”‚
â”‚  Eviction: Evicted before Guaranteed                            â”‚
â”‚                                                                  â”‚
â”‚  resources:                                                     â”‚
â”‚    requests:                                                    â”‚
â”‚      cpu: "100m"                                                â”‚
â”‚      memory: "128Mi"                                            â”‚
â”‚    limits:                                                      â”‚
â”‚      cpu: "200m"          â—„â”€â”€ Higher than request              â”‚
â”‚      memory: "256Mi"      â—„â”€â”€ Higher than request              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      BESTEFFORT                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Condition: NO requests or limits set                           â”‚
â”‚  Priority: LOWEST                                               â”‚
â”‚  Eviction: First to be evicted under pressure                   â”‚
â”‚                                                                  â”‚
â”‚  resources: {}          â—„â”€â”€ Nothing specified                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What Happens When Limits Are Exceeded?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CPU LIMIT EXCEEDED                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Container tries to use more CPU than limit                     â”‚
â”‚                     â”‚                                            â”‚
â”‚                     â–¼                                            â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚          â”‚   CPU THROTTLING     â”‚                               â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                     â”‚                                            â”‚
â”‚                     â–¼                                            â”‚
â”‚  â€¢ Container slowed down (not killed)                           â”‚
â”‚  â€¢ Process continues running                                    â”‚
â”‚  â€¢ Performance degraded                                         â”‚
â”‚  â€¢ Visible in metrics as throttling                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MEMORY LIMIT EXCEEDED                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Container tries to use more memory than limit                  â”‚
â”‚                     â”‚                                            â”‚
â”‚                     â–¼                                            â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚          â”‚     OOMKilled        â”‚                               â”‚
â”‚          â”‚ (Out Of Memory)      â”‚                               â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                     â”‚                                            â”‚
â”‚                     â–¼                                            â”‚
â”‚  â€¢ Container immediately terminated                             â”‚
â”‚  â€¢ Pod status: CrashLoopBackOff                                 â”‚
â”‚  â€¢ Event: "OOMKilled"                                           â”‚
â”‚  â€¢ Pod restarts automatically                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pod Eviction Under Resource Pressure

```
Scenario: Node running low on memory

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     NODE UNDER PRESSURE                       â”‚
â”‚                                                               â”‚
â”‚  Available Memory: 512Mi (critically low!)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Kubelet Evaluates     â”‚
              â”‚   Eviction Candidates   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                           â”‚
              â–¼                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  BestEffort Pods â”‚      â”‚  Burstable Pods  â”‚
    â”‚   (No limits)    â”‚      â”‚ (Using > request)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                         â”‚
             â–¼ EVICTED FIRST           â–¼ EVICTED SECOND
                                       
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚ Guaranteed Pods  â”‚
                      â”‚ (Safe - Last)    â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–²
                              â”‚
                     Protected from eviction
```

### Best Practices

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SETTING REQUESTS & LIMITS                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. START WITH MONITORING
   â”‚
   â”œâ”€â–º Run app without limits
   â”œâ”€â–º Monitor actual usage: kubectl top pod
   â”œâ”€â–º Use metrics over days/weeks
   â””â”€â–º Understand baseline and peaks

2. SET REQUESTS BASED ON BASELINE
   â”‚
   â”œâ”€â–º requests = average usage
   â”œâ”€â–º Ensures predictable scheduling
   â””â”€â–º Example: If avg is 80m CPU â†’ set request: "100m"

3. SET LIMITS WITH HEADROOM
   â”‚
   â”œâ”€â–º limits = peak usage + safety margin (20-50%)
   â”œâ”€â–º CPU: 2-4x requests (throttling is OK)
   â”œâ”€â–º Memory: 1.5-2x requests (OOMKill is NOT OK)
   â””â”€â–º Example: peak 180m CPU â†’ set limit: "250m"

4. NEVER SKIP MEMORY LIMITS
   â”‚
   â”œâ”€â–º Memory leaks can crash nodes
   â”œâ”€â–º Always set limits for production
   â””â”€â–º Monitor OOMKilled events

5. USE RESOURCE QUOTAS
   â”‚
   â”œâ”€â–º Set namespace-level quotas
   â”œâ”€â–º Prevent resource exhaustion
   â””â”€â–º Force teams to set requests/limits
```

### Common Scenarios

| Scenario | Configuration | Result |
|----------|--------------|--------|
| **Development Pod** | No requests/limits | BestEffort QoS, can use any free resources, first to be evicted |
| **Microservice (Prod)** | requests: 100m CPU, 128Mi<br>limits: 500m CPU, 512Mi | Burstable QoS, guaranteed minimum, can burst to handle spikes |
| **Database** | requests = limits<br>cpu: 2, memory: 4Gi | Guaranteed QoS, predictable performance, last to be evicted |
| **Batch Job** | requests: 500m CPU, 1Gi<br>limits: 2 CPU, 2Gi | Burstable, can use extra resources when available |
| **Memory Leak** | limits: memory: 256Mi | Pod killed with OOMKilled when exceeding 256Mi |

---

## ğŸ¯ Goal

- Understand how resource requests and limits affect Pod scheduling
- Simulate high CPU/memory usage and observe throttling or eviction
- Tune limits to balance performance and stability

## ğŸ§© Skills

- Scheduling
- Resource quotas
- Performance tuning
- Troubleshooting OOMKilled & Throttling

## ğŸ§° Scenario

We'll deploy a simple CPU-intensive Node.js app and apply various requests/limits to see how Kubernetes behaves when the container exceeds its limits.

## Step 1 â€” App That Uses CPU

### File: `cpu-app.js`

```javascript

const express = require('express');
const app = express();
const port = 8080;

// Global array to simulate memory usage
const memoryLoad = [];

// Endpoint to simulate CPU load
app.get('/load', (req, res) => {
  const end = Date.now() + 10000; // 10s busy loop
  while (Date.now() < end) {
    Math.sqrt(Math.random());
  }
  res.send('CPU load simulated for 10 seconds!');
});

// Endpoint to simulate memory load
app.get('/memload', (req, res) => {
  // Allocate ~10MB per request
  const size = 10 * 1024 * 1024 / 8; // Number of floats
  for (let i = 0; i < size; i++) {
    memoryLoad.push(Math.random());
  }
  res.send(`Allocated more memory! Current array length: ${memoryLoad.length}`);
});

// Root endpoint
app.get('/', (req, res) => res.send('Hello from CPU + Memory test app!'));

// Start server
app.listen(port, () => console.log(`App running on port ${port}`));

```

### File: `Dockerfile`

```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY cpu-app.js .
RUN npm install express
EXPOSE 8080
CMD ["node", "cpu-app.js"]
```

### Build and Push

```bash
docker build -t <your-dockerhub-user>/cpu-demo:v1 .
docker push <your-dockerhub-user>/cpu-demo:v1
```

## Step 2 â€” Deployment with Resource Requests/Limits

### File: `deployment-resources.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cpu-demo
  labels:
    app: cpu-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cpu-demo
  template:
    metadata:
      labels:
        app: cpu-demo
    spec:
      containers:
      - name: cpu-demo
        image: <your-dockerhub-user>/cpu-demo:v1
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "200m"
            memory: "256Mi"
```

### Apply

```bash
kubectl apply -f deployment-resources.yaml
kubectl get pods -l app=cpu-demo
```

## Step 3 â€” Expose the App

### File: `svc-cpu.yaml`

```yaml
apiVersion: v1
kind: Service
metadata:
  name: cpu-demo-svc
spec:
  selector:
    app: cpu-demo
  ports:
    - port: 80
      targetPort: 8080
  type: NodePort
```

### Apply

```bash
kubectl apply -f svc-cpu.yaml
kubectl get svc cpu-demo-svc
```

## Step 4 â€” Generate Load

### Get Node IP and test

```bash
curl http://<NodeIP>:<NodePort>/
curl http://<NodeIP>:<NodePort>/load
```

### Monitor CPU usage

While load runs, open another terminal:

```bash

#Install Metrics Server
#1.Install Metrics Server
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
#2.Patch Metrics Server (REQUIRED on Killercoda)
kubectl patch deployment metrics-server -n kube-system \
  --type=json \
  -p='[
    {"op":"add","path":"/spec/template/spec/containers/0/args/-","value":"--kubelet-insecure-tls"}
  ]'
#3.Wait until it's running
kubectl get pods -n kube-system | grep metrics

#################
kubectl top pod -l app=cpu-demo
```

**Observe:** CPU usage. Even if the process tries to exceed 200m CPU, it'll be throttled by the kubelet.

## Step 5 â€” Test Memory Limit

### Edit deployment to make memory limit too low

```bash
kubectl edit deployment cpu-demo
```

Change to:

```yaml
limits:
  cpu: "200m"
  memory: "64Mi"
```

### Apply changes

```bash
kubectl rollout restart deployment cpu-demo
```

### Simulate load repeatedly

```bash
for i in {1..5}; do curl http://<NodeIP>:<NodePort>/memload; done
```

### Inspect the results

```bash
kubectl describe pod -l app=cpu-demo | grep -A3 "State"
kubectl get events --sort-by=.metadata.creationTimestamp
```

You'll likely see:

```
Reason: OOMKilled
```

ğŸ’¡ **This means the container exceeded its memory limit and was terminated.**

## Step 6 â€” Verify Scheduling Behavior

### Try to schedule a pod that requests too many resources

### File: `heavy-pod.yaml`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: heavy-pod
spec:
  containers:
  - name: heavy
    image: busybox
    command: ["sh", "-c", "sleep 3600"]
    resources:
      requests:
        cpu: "10"
        memory: "10Gi"
```

### Apply

```bash
kubectl apply -f heavy-pod.yaml
kubectl get pod heavy-pod -o wide
```

You'll see:

```
Status: Pending
Reason: Unschedulable
```

**This means the scheduler can't find a node with enough resources.**

## Step 7 â€” Cleanup

```bash
kubectl delete deployment cpu-demo
kubectl delete svc cpu-demo-svc
kubectl delete pod heavy-pod
```

## ğŸ’¡ Tips

- **Use requests** for predictable scheduling
- **Use limits** to avoid noisy-neighbor issues
- **Monitor real usage** with `kubectl top pod` or metrics-server
- **In production**, define ResourceQuotas per namespace

## ğŸ“š What You Learned

- How resource requests affect Pod scheduling decisions
- How resource limits prevent containers from consuming excessive resources
- What happens when containers exceed memory limits (OOMKilled)
- How CPU throttling works when limits are exceeded
- How to troubleshoot scheduling issues related to insufficient resources
- Best practices for setting requests and limits in production environments
